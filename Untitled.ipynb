{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n",
    "y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n",
    "\n",
    "w1_auto = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
    "w2_auto= torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "w1_manual = w1_auto.clone()\n",
    "w2_manual = w2_auto.clone()\n",
    "\n",
    "\n",
    "b1_auto = torch.randn(1, 2, device=device, dtype=dtype, requires_grad=True)\n",
    "b2_auto = torch.randn(1, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "b1_manual = b1_auto.clone()\n",
    "b2_manual = b2_auto.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 73.56558227539062\n",
      "B1 [15.048255   1.0010177] B2 [67.00683] \n",
      "\n",
      "LOSS: 70.79010772705078\n",
      "B1 [13.287901    0.94068897] B2 [63.19552] \n",
      "\n",
      "LOSS: 68.35774993896484\n",
      "B1 [11.740544    0.88444906] B2 [59.629677] \n",
      "\n",
      "LOSS: 66.22496795654297\n",
      "B1 [10.379638  0.831991] B2 [56.292084] \n",
      "\n",
      "LOSS: 64.35396575927734\n",
      "B1 [9.182055   0.78302985] B2 [53.166954] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass:\n",
    "\n",
    "    h_1 = x.mm(w1_manual) + b1_manual\n",
    "    h_relu = h_1.clamp(min=0)\n",
    "    out = h_relu.mm(w2_manual) + b2_manual\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (out - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print('LOSS:',loss)\n",
    "    \n",
    "    # Backward pass: \n",
    "    dloss_dout = 2 * (out - y)\n",
    "    \n",
    "    grad_w2_manual = h_relu.T.mm(dloss_dout) \n",
    "    \n",
    "    grad_h_relu = dloss_dout.mm(w2_manual.T)\n",
    "    \n",
    "    grad_h_relu[h_1 < 0] = 0\n",
    "    \n",
    "    grad_w1_manual = x.T.mm(grad_h_relu)\n",
    "    \n",
    "    grad_b1_manual = grad_h_relu\n",
    "    grad_b2_manual = dloss_dout\n",
    "       \n",
    "    with torch.no_grad():\n",
    "        w1_manual -= learning_rate * grad_w1_manual\n",
    "        w2_manual -= learning_rate * grad_w2_manual\n",
    "        \n",
    "        b1_manual -= learning_rate * torch.sum(grad_b1_manual, dim=0)\n",
    "        b2_manual -= learning_rate * torch.sum(grad_b2_manual, dim=0)\n",
    "        \n",
    "    if t % 100 == 99:\n",
    "        print('B1',torch.sum(grad_b1_manual, dim=0).detach().numpy(), 'B2',torch.sum(grad_b2_manual, dim=0).detach().numpy(),'\\n')\n",
    "#         print(grad_w1_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 73.56558227539062\n",
      "B1 [[15.048255   1.0010177]] B2 [67.00683] \n",
      "\n",
      "LOSS: 70.79010772705078\n",
      "B1 [[13.287901    0.94068897]] B2 [63.19552] \n",
      "\n",
      "LOSS: 68.35774993896484\n",
      "B1 [[11.740544    0.88444906]] B2 [59.629677] \n",
      "\n",
      "LOSS: 66.22496795654297\n",
      "B1 [[10.379638  0.831991]] B2 [56.292084] \n",
      "\n",
      "LOSS: 64.35396575927734\n",
      "B1 [[9.182055   0.78302985]] B2 [53.166954] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(500):\n",
    "    y_pred = (x.mm(w1_auto) + b1_auto).clamp(min=0).mm(w2_auto) + b2_auto\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print('LOSS:', loss.item())\n",
    "    \n",
    "    # Теперь подсчет градиентов для весов происходит при вызове backward\n",
    "    loss.backward()\n",
    "   \n",
    "    # Обновляем значение весов, но укзаываем, чтобы PyTorch не считал эту операцию, \n",
    "    # которая бы учавствовала бы при подсчете градиентов в chain rule\n",
    "    with torch.no_grad():\n",
    "        w1_auto -= learning_rate * w1_auto.grad\n",
    "        w2_auto -= learning_rate * w2_auto.grad\n",
    "        b1_auto -= learning_rate * b1_auto.grad\n",
    "        b2_auto -= learning_rate * b2_auto.grad\n",
    "        \n",
    "        if t % 100 == 99:\n",
    "#             print(b1.grad, b2.grad)\n",
    "#             print(w1_auto.grad)\n",
    "            print('B1', b1_auto.grad.detach().numpy(), 'B2', b2_auto.grad.detach().numpy(), '\\n')\n",
    "        # Теперь обнуляем значение градиентов, чтобы на следующем шаге \n",
    "        # они не учитывались при подсчете новых градиентов,\n",
    "        # иначе произойдет суммирвоание старых и новых градиентов\n",
    "        w1_auto.grad.zero_()\n",
    "        w2_auto.grad.zero_()\n",
    "        b1_auto.grad.zero_()\n",
    "        b2_auto.grad.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаются одинаковые градиенты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
